{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **01.What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "Ensemble learning in machine learning combines predictions from multiple individual models (also known as base learners or weak learners) to create a more accurate and robust prediction than any single model alone. The core idea is that by aggregating the outputs of diverse models, the ensemble can overcome individual model limitations and achieve better performance.\n",
        "\n",
        "Ensemble learning is a method where we use many small models instead of just one. Each of these models may not be very strong on its own, but when we put their results together, we get a better and more accurate answer. It's like asking a group of people for advice instead of just one person—each one might be a little wrong, but together, they usually give a better answer.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "Ensemble Learning - GeeksforGeeks\n",
        "Instead of relying on a single model, ensemble learning leverages the collective intelligence of multiple models. These models can be trained on different subsets of the data, use different algorithms, or have different hyperparameter settings. By combining their predictions, the ensemble can reduce variance, bias, and overfitting, leading to improved accuracy and generalization.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. Base Learners:\n",
        "\n",
        "Individual machine learning models are trained on the data. These models can be decision trees, neural networks, support vector machines, or any other suitable algorithm.\n",
        "\n",
        "2. Ensemble Creation:\n",
        "\n",
        "The base learners are combined to create an ensemble. Common methods for combining predictions include:\n",
        "Bagging: Training multiple models independently on random subsets of the data (with replacement) and averaging their predictions.\n",
        "\n",
        "Boosting: Training models sequentially, with each model focusing on correcting the errors of its predecessors.\n",
        "\n",
        "Stacking: Training multiple models and then training a meta-model on their predictions to make the final prediction.\n",
        "\n",
        "3. Prediction:\n",
        "\n",
        "When a new data point needs to be classified or predicted, each base learner in the ensemble makes a prediction. These predictions are then combined, often by averaging or voting, to produce the final ensemble prediction.\n",
        "\n",
        "Benefits of Ensemble Learning:\n",
        "\n",
        "Increased Accuracy: Ensemble methods often achieve higher accuracy than individual models.\n",
        "Reduced Variance: By combining multiple models, the overall variance of the predictions is reduced, leading to more stable results.\n",
        "\n",
        "Improved Generalization:\n",
        "\n",
        "Ensembles can generalize better to unseen data, reducing the risk of overfitting.\n",
        "Robustness: Ensembles are more robust to noise and outliers in the data.\n",
        "\n",
        "Benefits of Ensemble Learning in Machine Learning\n",
        "Ensemble learning is a versatile approach that can be applied to machine learning model for: -\n",
        "\n",
        "Reduction in Overfitting:\n",
        "\n",
        "By aggregating predictions of multiple model's ensembles can reduce overfitting that individual complex models might exhibit.\n",
        "\n",
        "Improved Generalization:\n",
        "\n",
        " It generalizes better to unseen data by minimizing variance and bias.\n",
        "\n",
        "Increased Accuracy:\n",
        "\n",
        "Combining multiple models gives higher predictive accuracy.\n",
        "\n",
        "Robustness to Noise:\n",
        "\n",
        " It mitigates the effect of noisy or incorrect data points by averaging out predictions from diverse models.\n",
        "\n",
        "Flexibility:\n",
        "\n",
        "It can work with diverse models including decision trees, neural networks and support vector machines making them highly adaptable.\n",
        "\n",
        "Bias-Variance Tradeoff:\n",
        "\n",
        "Techniques like bagging reduce variance, while boosting reduces bias leading to better overall performance.\n"
      ],
      "metadata": {
        "id": "miy1nHfJTuH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **02.What is the difference between Bagging and Boosting?**\n",
        "\n",
        "\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Bagging is a popular ensemble learning technique that focuses on reducing variance and improving the stability of machine learning models. The term “bagging” is derived from the idea of creating multiple subsets or bags of the training data through a process known as bootstrapping. Bootstrapping involves randomly sampling the dataset with replacement to generate multiple subsets of the same size as the original data. Each of these subsets is then used to train a base learner independently.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Boosting, like bagging, is an ensemble learning technique, but it aims to improve the performance of weak learners by combining them in a sequential manner. The core idea behind boosting is to give more weight to misclassified instances during the training process, enabling subsequent learners to focus on the mistakes made by their predecessors.\n",
        "\n",
        "Differences Between Bagging and Boosting:\n",
        "\n",
        "Sequential vs. Parallel:\n",
        "\n",
        "Bagging:\n",
        "\n",
        " The base learners are trained independently in parallel, as each learner works on a different subset of the data. The final prediction is typically an average or vote of all base learners.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "The base learners are trained sequentially, and each learner focuses on correcting the mistakes of its predecessors. The final prediction is a weighted sum of the individual learner predictions.\n",
        "\n",
        "Data Sampling:\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Utilizes bootstrapping to create multiple subsets of the training data, allowing for variations in the training sets for each base learner.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Assigns weights to instances in the training set, with higher weights given to misclassified instances to guide subsequent learners.\n",
        "\n",
        "Weighting of Base Learners:\n",
        "\n",
        "Bagging:\n",
        "\n",
        "All base learners typically have equal weight when making the final prediction.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Assigns different weights to each base learner based on its performance, giving more influence to learners that perform well on challenging instances.\n",
        "\n",
        "Handling Noisy Data and Outliers:\n",
        "\n",
        "Bagging:\n",
        "\n",
        " Robust to noisy data and outliers due to the averaging or voting mechanism, which reduces the impact of individual errors.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "More sensitive to noisy data and outliers, as the focus on misclassified instances might lead to overfitting on these instances.\n",
        "\n",
        "Model Diversity:\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Aims to create diverse base learners through random subsets of the data and, in the case of Random Forests, random feature selection for each tree.\n",
        "\n",
        "Boosting:\n",
        "\n",
        " Focuses on improving the performance of weak learners sequentially, with each learner addressing the weaknesses of its predecessors.\n",
        "\n",
        "Bias and Variance:\n",
        "\n",
        "Bagging:\n",
        "\n",
        " Primarily reduces variance by averaging predictions from multiple models, making it effective for models with high variance.\n",
        "\n",
        "Boosting:\n",
        "\n",
        " Addresses both bias and variance, with a focus on reducing bias by sequentially correcting mistakes made by weak learners.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fziijEtiVI-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **03.What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "\n",
        "Bootstrap sampling is a resampling technique where multiple samples of the same size as the original dataset are created by randomly selecting data points with replacement. This means that a single data point can be selected multiple times within a single bootstrap sample, and some data points from the original dataset may not be included in a particular bootstrap sample at all.\n",
        "\n",
        "Bagging Classifier :-\n",
        "\n",
        "Bagging or Bootstrap aggregating is a type of ensemble learning in which multiple base models are trained independently and parallelly on different subsets of training data. Each subset is generated using bootstrap sampling in which data points are picked at randomly with replacement. In bagging classifier the final prediction is made by aggregating the predictions of all base model using majority voting. In the models of regression the final prediction is made by averaging the predictions of the all base model and that is known as bagging regression.\n",
        "\n",
        "In Bagging (Bootstrap Aggregating) methods like Random Forest, bootstrap sampling plays a crucial role:\n",
        "\n",
        "Creating Diverse Training Sets:\n",
        "\n",
        "Each individual model (e.g., decision tree in a Random Forest) within the ensemble is trained on a different bootstrap sample. This introduces variability among the training sets, which in turn leads to the creation of diverse base models.\n",
        "\n",
        "Reducing Variance and Overfitting:\n",
        "\n",
        "By training multiple models on these varied bootstrap samples and then aggregating their predictions (e.g., through majority voting for classification or averaging for regression), Bagging methods effectively reduce the variance of the overall model. This helps in mitigating overfitting, as the ensemble is less sensitive to the specific characteristics of any single training sample.\n",
        "\n",
        "Enhancing Model Robustness:\n",
        "\n",
        "The diversity introduced by bootstrap sampling makes the ensemble more robust to noise and outliers in the data. Since each base model is trained on a slightly different view of the data, the combined prediction is less likely to be swayed by anomalies present in only a subset of the data.\n",
        "\n",
        "In essence, bootstrap sampling provides the foundation for creating a collection of slightly different base models, which are then combined to form a more stable, accurate, and robust ensemble model in techniques such as Random Forest.\n",
        "\n",
        "How does Bagging Classifier Work :-\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "In Bootstrap Sampling data are sampled with 'n' subsets are made randomly from original training dataset with replacement. This step ensures that the base models are trained on diverse subsets of the data as some samples may appear multiple times in the new subset while others may be left out. It reduces the risks of overfitting and improves the accuracy of the model.\n",
        "\n",
        "Base Model Training:\n",
        "\n",
        "In bagging multiple base models are used. After the Bootstrap Sampling each base model is independently trained using learning algorithm such as decision trees, support vector machines or neural networks on a different bootstrapped subset data. These models are typically called \"Weak learners\" because they are not highly accurate. Since the base model is trained independently and parallelly it makes it computationally efficient and time saving.\n",
        "\n",
        "Aggregation:\n",
        "\n",
        "Once all the base models are trained and makes predictions on new unseen data then bagging classifier predicts class label for given instance by majority voting from all base learners. The class which has the majority voting is the prediction of the model.\n",
        "\n",
        "Out-of-Bag (OOB) Evaluation:\n",
        "\n",
        "Some samples are excluded from the training subset of particular base models during the bootstrapping method. These \"out-of-bag\" samples can be used to estimate the model's performance without the need for cross-validation.\n",
        "\n",
        "Bagging Classifier process begins with the original training dataset which is used to create bootstrap samples (random subsets with replacement) for training multiple weak learners ensuring diversity. Each weak learner independently predicts outcomes as shown in the Base Model Training graph capturing different patterns.  \n",
        "\n",
        "These predictions are aggregated using majority voting where the final classification is determined by the maximum voted output. The Out-of-Bag (OOB) evaluates models performance on data excluded from each bootstrap sample for validation. This approach enhances accuracy and reduces overfitting."
      ],
      "metadata": {
        "id": "y0MRIsZUhnL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **04.What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "Out-of-bag (OOB) samples are data points that are not included in a specific bootstrap sample during the training of a random forest model. In other words, when creating multiple decision trees in a random forest, each tree is trained on a random subset of the data (with replacement), and the remaining data points that are not selected for that particular tree are considered OOB samples. These OOB samples can be used to estimate the model's performance without needing a separate validation set.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "Bootstrapping:\n",
        "\n",
        "Random forests use a technique called bootstrapping, where random samples with replacement are drawn from the original dataset to create multiple training sets for individual decision trees.\n",
        "\n",
        "OOB Samples as a Validation Set:\n",
        "\n",
        "Because of bootstrapping, some data points will not be included in the training data for a specific tree. These unselected data points are the OOB samples for that particular tree.\n",
        "\n",
        "OOB Error:\n",
        "\n",
        "The OOB samples can be used to estimate the model's generalization error (how well it performs on unseen data). By predicting the OOB samples for each tree and aggregating those predictions, you can get an OOB error, which serves as an estimate of the model's performance.\n",
        "\n",
        "Advantages of OOB Samples:\n",
        "\n",
        "Using OOB samples eliminates the need for a separate validation set, simplifying the model evaluation process. It also provides an unbiased estimate of the model's performance, as the OOB samples are not used in training the specific tree they are being evaluated on.\n",
        "\n",
        "The Out-of-Bag (OOB) score is a method used to evaluate the performance of ensemble models, particularly those that utilize bagging, such as Random Forests. It provides an internal, unbiased estimate of the model's generalization error without the need for a separate validation set.\n",
        "\n",
        "How OOB Score is Used:-\n",
        "\n",
        "Bootstrapped Sampling and OOB Samples:\n",
        "\n",
        "During the training of an ensemble model with bagging, each base learner (e.g., a decision tree in a Random Forest) is trained on a bootstrapped sample of the original dataset. A bootstrapped sample is created by randomly sampling with replacement from the original data. The data points that are not included in a particular bootstrapped sample for a specific base learner are called \"out-of-bag\" (OOB) samples for that base learner.\n",
        "\n",
        "Individual Base Learner Evaluation:\n",
        "\n",
        "Each base learner is then evaluated on its respective OOB samples. This means that for each data point in the original dataset, there will be a subset of base learners for which that data point was OOB.\n",
        "\n",
        "Aggregated OOB Predictions:\n",
        "\n",
        "For each original data point, predictions are made by all the base learners for which that data point was an OOB sample. These individual predictions are then aggregated (e.g., by majority vote for classification or averaging for regression) to form a final OOB prediction for that data point.\n",
        "\n",
        "OOB Score Calculation:\n",
        "\n",
        "The OOB score is calculated by comparing these aggregated OOB predictions to the actual values of the corresponding data points. For classification, this typically involves calculating the accuracy (proportion of correctly classified OOB samples). For regression, metrics like Mean Squared Error (MSE) or R-squared are commonly used on the OOB samples.\n",
        "\n",
        "Benefits of OOB Score:\n",
        "\n",
        "Unbiased Estimate:\n",
        "\n",
        "The OOB score provides an unbiased estimate of the model's performance on unseen data because the OOB samples were not used in the training of the specific base learners that are predicting on them.\n",
        "\n",
        "Efficient Cross-Validation:\n",
        "\n",
        "It acts as a form of internal cross-validation, eliminating the need to explicitly split the data into separate training and validation sets, thus maximizing the use of available data for training.\n",
        "\n",
        "Reduced Data Leakage:\n",
        "\n",
        "It helps in avoiding data leakage, which can occur when using traditional cross-validation methods if not implemented carefully.\n"
      ],
      "metadata": {
        "id": "3AcBEl9-hxBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **05.Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "Feature importance analysis in a single Decision Tree and a Random Forest differs primarily due to the ensemble nature of Random Forests.\n",
        "\n",
        "Single Decision Tree:-\n",
        "\n",
        "Decision Tree is very popular supervised machine learning algorithm used for regression as well as classification problems. In decision tree, a flow-chart like structure is build where each internal nodes denotes the features, rules are denoted using the branches and the leaves denotes the final result of the algorithm.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "Feature importance in a single decision tree is typically calculated based on how much each feature reduces impurity (e.g., Gini impurity or entropy) when used for splitting nodes. Features that lead to larger reductions in impurity are considered more important.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Feature importance in a single tree is straightforward to interpret and visualize, as it directly reflects the tree's decision-making process.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "A single decision tree can be prone to overfitting, and its feature importance may be highly sensitive to small changes in the training data. If two features are highly correlated, the tree might only pick one for splitting, artificially diminishing the importance of the other.\n",
        "\n",
        "Random Forest:-\n",
        "\n",
        "Random Forest is very powerful supervised machine learning algorithm, used for classification and regression task. Random Forest uses ensemble learning (combining multiple models/classifiers to solve a complex problem and to improve the overall accuracy score of the model). In Random Forest multiple decision tree are built by considering the different subset of the given data and the average of all those to increase the overall accuracy of the model. As the number of decision tree in random forest increases the accuracy increases and overfitting also reduces.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "Random Forests calculate feature importance by averaging the impurity reduction across all individual decision trees within the forest (Mean Decrease in Impurity, MDI). Another common method is Permutation Importance, which measures the decrease in model performance when a feature's values are randomly shuffled.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "While less directly interpretable than a single tree's importance due to the ensemble, Random Forest feature importance provides a more robust and stable measure by aggregating insights from multiple trees.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Random Forests inherently mitigate the overfitting issues of single trees, leading to more generalized and reliable feature importance scores. They are better at handling correlated features, as different trees in the forest might utilize different correlated features, leading to a more balanced importance attribution.\n",
        "\n",
        "Considerations:\n",
        "\n",
        "Random Forest feature importance can still exhibit biases, such as favoring high-cardinality features or potentially understating the importance of correlated features if their contributions are split across multiple trees. Permutation importance often offers a more robust alternative in such cases.\n",
        "\n",
        "When to Use Random Forest vs. Decision Tree?\n",
        "\n",
        "*Use a decision tree when interpretability is important, and you need a simple and easy-to-understand model.\n",
        "\n",
        "*Use a random forest when you want better generalization performance, robustness to overfitting, and improved accuracy, especially on complex datasets with high-dimensional feature spaces.\n",
        "\n",
        "*If computational efficiency is a concern and you have a small dataset, a decision tree might be more appropriate.\n",
        "\n",
        "*If you have a large dataset with complex relationships between features and labels, a random forest is likely to provide better results.\n",
        "\n"
      ],
      "metadata": {
        "id": "eAtEwQEwsXtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#06.: Write a Python program to:\n",
        "\n",
        "#● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "import pandas as pd\n",
        "data_df = pd.DataFrame(data = data.data,\n",
        "                       columns = data.feature_names)\n",
        "data_df.head().T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "4we2D9KUwHVE",
        "outputId": "5776ce76-a967-4598-a51f-b698f8b6a34f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   0            1            2           3  \\\n",
              "mean radius                17.990000    20.570000    19.690000   11.420000   \n",
              "mean texture               10.380000    17.770000    21.250000   20.380000   \n",
              "mean perimeter            122.800000   132.900000   130.000000   77.580000   \n",
              "mean area                1001.000000  1326.000000  1203.000000  386.100000   \n",
              "mean smoothness             0.118400     0.084740     0.109600    0.142500   \n",
              "mean compactness            0.277600     0.078640     0.159900    0.283900   \n",
              "mean concavity              0.300100     0.086900     0.197400    0.241400   \n",
              "mean concave points         0.147100     0.070170     0.127900    0.105200   \n",
              "mean symmetry               0.241900     0.181200     0.206900    0.259700   \n",
              "mean fractal dimension      0.078710     0.056670     0.059990    0.097440   \n",
              "radius error                1.095000     0.543500     0.745600    0.495600   \n",
              "texture error               0.905300     0.733900     0.786900    1.156000   \n",
              "perimeter error             8.589000     3.398000     4.585000    3.445000   \n",
              "area error                153.400000    74.080000    94.030000   27.230000   \n",
              "smoothness error            0.006399     0.005225     0.006150    0.009110   \n",
              "compactness error           0.049040     0.013080     0.040060    0.074580   \n",
              "concavity error             0.053730     0.018600     0.038320    0.056610   \n",
              "concave points error        0.015870     0.013400     0.020580    0.018670   \n",
              "symmetry error              0.030030     0.013890     0.022500    0.059630   \n",
              "fractal dimension error     0.006193     0.003532     0.004571    0.009208   \n",
              "worst radius               25.380000    24.990000    23.570000   14.910000   \n",
              "worst texture              17.330000    23.410000    25.530000   26.500000   \n",
              "worst perimeter           184.600000   158.800000   152.500000   98.870000   \n",
              "worst area               2019.000000  1956.000000  1709.000000  567.700000   \n",
              "worst smoothness            0.162200     0.123800     0.144400    0.209800   \n",
              "worst compactness           0.665600     0.186600     0.424500    0.866300   \n",
              "worst concavity             0.711900     0.241600     0.450400    0.686900   \n",
              "worst concave points        0.265400     0.186000     0.243000    0.257500   \n",
              "worst symmetry              0.460100     0.275000     0.361300    0.663800   \n",
              "worst fractal dimension     0.118900     0.089020     0.087580    0.173000   \n",
              "\n",
              "                                   4  \n",
              "mean radius                20.290000  \n",
              "mean texture               14.340000  \n",
              "mean perimeter            135.100000  \n",
              "mean area                1297.000000  \n",
              "mean smoothness             0.100300  \n",
              "mean compactness            0.132800  \n",
              "mean concavity              0.198000  \n",
              "mean concave points         0.104300  \n",
              "mean symmetry               0.180900  \n",
              "mean fractal dimension      0.058830  \n",
              "radius error                0.757200  \n",
              "texture error               0.781300  \n",
              "perimeter error             5.438000  \n",
              "area error                 94.440000  \n",
              "smoothness error            0.011490  \n",
              "compactness error           0.024610  \n",
              "concavity error             0.056880  \n",
              "concave points error        0.018850  \n",
              "symmetry error              0.017560  \n",
              "fractal dimension error     0.005115  \n",
              "worst radius               22.540000  \n",
              "worst texture              16.670000  \n",
              "worst perimeter           152.200000  \n",
              "worst area               1575.000000  \n",
              "worst smoothness            0.137400  \n",
              "worst compactness           0.205000  \n",
              "worst concavity             0.400000  \n",
              "worst concave points        0.162500  \n",
              "worst symmetry              0.236400  \n",
              "worst fractal dimension     0.076780  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33f9495e-54ec-4f29-84a3-4a9dd0e11c35\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>17.990000</td>\n",
              "      <td>20.570000</td>\n",
              "      <td>19.690000</td>\n",
              "      <td>11.420000</td>\n",
              "      <td>20.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>10.380000</td>\n",
              "      <td>17.770000</td>\n",
              "      <td>21.250000</td>\n",
              "      <td>20.380000</td>\n",
              "      <td>14.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>122.800000</td>\n",
              "      <td>132.900000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>77.580000</td>\n",
              "      <td>135.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1326.000000</td>\n",
              "      <td>1203.000000</td>\n",
              "      <td>386.100000</td>\n",
              "      <td>1297.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>0.118400</td>\n",
              "      <td>0.084740</td>\n",
              "      <td>0.109600</td>\n",
              "      <td>0.142500</td>\n",
              "      <td>0.100300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.078640</td>\n",
              "      <td>0.159900</td>\n",
              "      <td>0.283900</td>\n",
              "      <td>0.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>0.300100</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.241400</td>\n",
              "      <td>0.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>0.147100</td>\n",
              "      <td>0.070170</td>\n",
              "      <td>0.127900</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>0.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean symmetry</th>\n",
              "      <td>0.241900</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.259700</td>\n",
              "      <td>0.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <td>0.078710</td>\n",
              "      <td>0.056670</td>\n",
              "      <td>0.059990</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>0.058830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius error</th>\n",
              "      <td>1.095000</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>0.745600</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>0.757200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture error</th>\n",
              "      <td>0.905300</td>\n",
              "      <td>0.733900</td>\n",
              "      <td>0.786900</td>\n",
              "      <td>1.156000</td>\n",
              "      <td>0.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter error</th>\n",
              "      <td>8.589000</td>\n",
              "      <td>3.398000</td>\n",
              "      <td>4.585000</td>\n",
              "      <td>3.445000</td>\n",
              "      <td>5.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area error</th>\n",
              "      <td>153.400000</td>\n",
              "      <td>74.080000</td>\n",
              "      <td>94.030000</td>\n",
              "      <td>27.230000</td>\n",
              "      <td>94.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness error</th>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.011490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness error</th>\n",
              "      <td>0.049040</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.040060</td>\n",
              "      <td>0.074580</td>\n",
              "      <td>0.024610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity error</th>\n",
              "      <td>0.053730</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.038320</td>\n",
              "      <td>0.056610</td>\n",
              "      <td>0.056880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points error</th>\n",
              "      <td>0.015870</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.020580</td>\n",
              "      <td>0.018670</td>\n",
              "      <td>0.018850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry error</th>\n",
              "      <td>0.030030</td>\n",
              "      <td>0.013890</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>0.059630</td>\n",
              "      <td>0.017560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal dimension error</th>\n",
              "      <td>0.006193</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>0.005115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst radius</th>\n",
              "      <td>25.380000</td>\n",
              "      <td>24.990000</td>\n",
              "      <td>23.570000</td>\n",
              "      <td>14.910000</td>\n",
              "      <td>22.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst texture</th>\n",
              "      <td>17.330000</td>\n",
              "      <td>23.410000</td>\n",
              "      <td>25.530000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>16.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst perimeter</th>\n",
              "      <td>184.600000</td>\n",
              "      <td>158.800000</td>\n",
              "      <td>152.500000</td>\n",
              "      <td>98.870000</td>\n",
              "      <td>152.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst area</th>\n",
              "      <td>2019.000000</td>\n",
              "      <td>1956.000000</td>\n",
              "      <td>1709.000000</td>\n",
              "      <td>567.700000</td>\n",
              "      <td>1575.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst smoothness</th>\n",
              "      <td>0.162200</td>\n",
              "      <td>0.123800</td>\n",
              "      <td>0.144400</td>\n",
              "      <td>0.209800</td>\n",
              "      <td>0.137400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst compactness</th>\n",
              "      <td>0.665600</td>\n",
              "      <td>0.186600</td>\n",
              "      <td>0.424500</td>\n",
              "      <td>0.866300</td>\n",
              "      <td>0.205000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concavity</th>\n",
              "      <td>0.711900</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>0.450400</td>\n",
              "      <td>0.686900</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concave points</th>\n",
              "      <td>0.265400</td>\n",
              "      <td>0.186000</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>0.257500</td>\n",
              "      <td>0.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst symmetry</th>\n",
              "      <td>0.460100</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.361300</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <td>0.118900</td>\n",
              "      <td>0.089020</td>\n",
              "      <td>0.087580</td>\n",
              "      <td>0.173000</td>\n",
              "      <td>0.076780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33f9495e-54ec-4f29-84a3-4a9dd0e11c35')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33f9495e-54ec-4f29-84a3-4a9dd0e11c35 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33f9495e-54ec-4f29-84a3-4a9dd0e11c35');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-07a8b677-250b-43c4-9972-fa1d43969372\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07a8b677-250b-43c4-9972-fa1d43969372')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-07a8b677-250b-43c4-9972-fa1d43969372 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_df"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#06.Write a Python program to:\n",
        "\n",
        " #using sklearn.datasets.load_breast_cancer()\n",
        " #●Train a Random Forest Classifier\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = breast_cancer_data.data\n",
        "y = breast_cancer_data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size=0.2 means 20% of the data will be used for testing\n",
        "# random_state ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "# n_estimators is the number of trees in the forest\n",
        "# random_state ensures reproducibility of the model training\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier on the training data\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = random_forest_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, target_names=breast_cancer_data.target_names)\n",
        "\n",
        "print(f\"Accuracy of the Random Forest Classifier: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtiwRC2UxQGv",
        "outputId": "042e4ebb-ac2f-4590-c04e-c3f16894b9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Random Forest Classifier: 0.9649\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.93      0.95        43\n",
            "      benign       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#06.Write a Python program using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "#●Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Train a RandomForestClassifier model\n",
        "# n_estimators: The number of trees in the forest.\n",
        "# random_state: Controls the randomness of the estimator.\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to store feature names and their importance scores\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': breast_cancer.feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 most important features:\")\n",
        "print(importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydqt9AliyM6e",
        "outputId": "a3d36c44-1c60-4521-a9d9-080932390afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#07.Write a Python program to:\n",
        "\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a Decision Tree Classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier\n",
        "# n_estimators: Number of base estimators (Decision Trees) in the ensemble\n",
        "# base_estimator: The individual estimator to be bagged\n",
        "# random_state: For reproducibility\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    estimator=base_estimator,\n",
        "    n_estimators=10,  # You can adjust the number of estimators\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Bagging Classifier: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUEaZGMJyqH1",
        "outputId": "b49dc2e3-c759-41b2-80ff-eac67a0d8cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#07.Write a Python program to:\n",
        "\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load a sample dataset\n",
        "#    Using the Iris dataset for demonstration purposes\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target variable\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train a single Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the accuracy of the Decision Tree\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Accuracy of the single Decision Tree: {accuracy_dt:.4f}\")\n",
        "\n",
        "# 6. (Optional) Compare with another model (e.g., another Decision Tree with different parameters)\n",
        "#    For comparison, let's train another Decision Tree with a different criterion\n",
        "dt_classifier_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier_gini.fit(X_train, y_train)\n",
        "y_pred_gini = dt_classifier_gini.predict(X_test)\n",
        "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "print(f\"Accuracy of the Gini-based Decision Tree: {accuracy_gini:.4f}\")\n",
        "\n",
        "# 7. Comparison statement\n",
        "if accuracy_dt > accuracy_gini:\n",
        "    print(\"The default Decision Tree performed slightly better or equally well in this case.\")\n",
        "elif accuracy_gini > accuracy_dt:\n",
        "    print(\"The Gini-based Decision Tree performed slightly better in this case.\")\n",
        "else:\n",
        "    print(\"Both Decision Trees achieved the same accuracy.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkdvVfRWzBnx",
        "outputId": "f6f2fe96-a220-4078-9277-fcc37c33f55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the single Decision Tree: 1.0000\n",
            "Accuracy of the Gini-based Decision Tree: 1.0000\n",
            "Both Decision Trees achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#08.Write a Python program to:\n",
        "\n",
        "#● Train a Random Forest Classifier\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Prepare a sample dataset (replace with your actual data)\n",
        "# For demonstration, we'll create a synthetic dataset\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "y = np.random.randint(0, 2, 100) # 100 binary labels (0 or 1)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Instantiate the Random Forest Classifier\n",
        "# n_estimators: number of trees in the forest\n",
        "# random_state: for reproducibility of results\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Random Forest Classifier: {accuracy:.2f}\")\n",
        "\n",
        "# You can also explore other metrics like:\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# print(classification_report(y_test, y_pred))\n",
        "# print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "q76B-rc1znXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4627ec-aaea-4ef3-bb52-bbf94121a45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Random Forest Classifier: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#08.Write a Python program to:\n",
        "\n",
        "#Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# 1. Prepare data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, 20],\n",
        "    'n_estimators': [50, 100, 150, 200]\n",
        "}\n",
        "\n",
        "# 3. Instantiate the estimator\n",
        "estimator = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# 5. Fit GridSearchCV\n",
        "print(\"Performing Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Grid Search complete.\")\n",
        "\n",
        "# 6. Retrieve best parameters and score\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
        "\n",
        "# You can also access the best estimator directly\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"\\nBest Estimator:\", best_model)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test Set Accuracy of Best Model:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqTLZs0ACF00",
        "outputId": "5fecad55-2c30-4903-eee0-262ee489fed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Grid Search...\n",
            "Grid Search complete.\n",
            "\n",
            "Best Parameters: {'max_depth': 15, 'n_estimators': 100}\n",
            "Best Cross-validation Score: 0.9275\n",
            "\n",
            "Best Estimator: RandomForestClassifier(max_depth=15, random_state=42)\n",
            "Test Set Accuracy of Best Model: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#08.Write a Python program to:\n",
        "\n",
        "#Print the best parameters and final accuracy\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier # Example model\n",
        "\n",
        "# Assume X_train, y_train, X_test, y_test are already defined\n",
        "# (e.g., from train_test_split)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by Grid Search\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model with best parameters)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the final accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNxfayBpChuk",
        "outputId": "31e0d6cb-21fa-48e9-ee2f-2f40411d4891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Test set accuracy: 0.945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#09.Write a Python program to:\n",
        "\n",
        "#Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "#Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Bagging Regressor ---\n",
        "# A Bagging Regressor with a Decision Tree as the base estimator\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=100,  # Number of base estimators (trees)\n",
        "    max_samples=0.8,   # Use 80% of the samples for each base estimator\n",
        "    bootstrap=True,    # Sample with replacement\n",
        "    random_state=42,\n",
        "    n_jobs=-1          # Use all available CPU cores\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Regressor Mean Squared Error: {mse_bagging:.4f}\")\n",
        "\n",
        "# --- Random Forest Regressor ---\n",
        "random_forest_reg = RandomForestRegressor(\n",
        "    n_estimators=100,  # Number of trees in the forest\n",
        "    max_features=0.8,  # Use 80% of features for each split\n",
        "    random_state=42,\n",
        "    n_jobs=-1          # Use all available CPU cores\n",
        ")\n",
        "random_forest_reg.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Regressor Mean Squared Error: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO9_gpOUDHn0",
        "outputId": "18ea3c2a-a230-4f0a-c688-2665a13722a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Mean Squared Error: 0.2599\n",
            "Random Forest Regressor Mean Squared Error: 0.2536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10.You are working as a data scientist at a financial institution to predict loan default.You have access to customer demographic and transaction history data.**\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "\n",
        "\n",
        "**ANSWER-**\n",
        "\n",
        "Here's a step-by-step approach to using ensemble techniques for loan default prediction:\n",
        "\n",
        "1.Choose between Bagging or Boosting:\n",
        "\n",
        "Consider Boosting (e.g., Gradient Boosting, XGBoost, LightGBM) for this problem.\n",
        "Loan default prediction often involves complex relationships and potentially imbalanced datasets. Boosting algorithms sequentially build models to correct errors of previous models, making them very effective at capturing intricate patterns and achieving high predictive accuracy, which is crucial in financial risk assessment. Bagging (e.g., Random Forest) is good for reducing variance and overfitting, but boosting often provides superior performance in complex classification tasks like this.\n",
        "\n",
        "2.Handle Overfitting:\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Apply L1 or L2 regularization techniques during model training to penalize large coefficients and prevent complex models that fit noise in the data.\n",
        "\n",
        "Cross-validation:\n",
        "\n",
        "Use k-fold cross-validation during model training to assess performance on unseen data and identify the optimal hyperparameters that prevent overfitting.\n",
        "\n",
        "Feature Engineering/Selection:\n",
        "\n",
        "Carefully select and engineer features to reduce dimensionality and focus on the most relevant information, minimizing the chances of the model learning spurious correlations.\n",
        "\n",
        "Early Stopping:\n",
        "\n",
        "For iterative algorithms like boosting, monitor performance on a validation set and stop training when performance on the validation set starts to degrade, preventing the model from memorizing the training data.\n",
        "\n",
        "3.Select Base Models:\n",
        "\n",
        "Decision Trees:\n",
        "\n",
        "These are commonly used as base learners in both bagging (e.g., Random Forest) and boosting (e.g., Gradient Boosting Machines). They are interpretable and can capture non-linear relationships.\n",
        "\n",
        "Consider variations:\n",
        "\n",
        "For boosting, explore different types of base learners like shallow decision trees (stumps) or even linear models if appropriate for certain features.\n",
        "\n",
        "4.Evaluate Performance using Cross-Validation:\n",
        "\n",
        "K-Fold Cross-Validation:\n",
        "\n",
        "Divide the dataset into 'k' folds. Train the model on 'k-1' folds and evaluate on the remaining fold. Repeat this 'k' times, using each fold once as the validation set. This provides a robust estimate of the model's generalization performance.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Use appropriate evaluation metrics for imbalanced datasets, such as AUC-ROC, Precision, Recall, F1-score, or Gini coefficient, rather than just accuracy, as accuracy can be misleading when one class is much larger than the other.\n",
        "\n",
        "5.Justify how ensemble learning improves decision-making in this real-world context:\n",
        "\n",
        "Increased Accuracy and Robustness:\n",
        "\n",
        "Ensemble methods, by combining multiple models, generally achieve higher predictive accuracy and are more robust to noise and outliers compared to single models. This leads to more reliable loan default predictions.\n",
        "\n",
        "Reduced Risk:\n",
        "\n",
        "More accurate predictions of loan default allow the financial institution to make better-informed decisions regarding loan approvals, interest rates, and risk management strategies, ultimately reducing financial losses due to defaults.\n",
        "\n",
        "Improved Decision Support:\n",
        "\n",
        "The ensemble model provides a more comprehensive and reliable assessment of loan applicant risk, enabling faster and more consistent decision-making for loan officers and automated systems.\n",
        "\n",
        "Handling Complexity:\n",
        "\n",
        "Ensemble methods can effectively capture complex, non-linear relationships within the customer demographic and transaction history data that might be missed by simpler models, providing a more nuanced understanding of default risk.\n"
      ],
      "metadata": {
        "id": "TyMRsjPLGlnk"
      }
    }
  ]
}